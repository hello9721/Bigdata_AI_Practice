```python
import numpy as np                    # tensorflow 는 3.9와 호환이 안되기에 3.8버전으로.
```


```python
import tensorflow as tf               # tensorflow 는 2.3.0 버전으로.
```


```python
import pandas as pd
```


```python

```


```python
import matplotlib.pyplot as plt
```


```python
%matplotlib inline
```


```python

```


```python
import seaborn as sns
```


```python
sns.set(style="darkgrid")
```


```python

```


```python
cols = ['price', 'maint', 'doors', 'persons', 'lug_capacity', 'safety', 'output']
```


```python
cars = pd.read_csv("C:/Users/user8/learning/source/chap2/data/car_evaluation.csv", names = cols, header=None)
```


```python
plot_size = plt.rcParams["figure.figsize"]
```


```python
plot_size
```



    [6.4, 4.8]




```python
cars.output.value_counts().plot(kind = 'pie', autopct="%0.05f%%", explode = (0.05,0.05,0.05,0.05))
```



    <AxesSubplot: ylabel='output'>



![output_14_1](https://user-images.githubusercontent.com/95046369/198939913-7fe8a53f-591d-4aac-86f2-b1aafc13ad9b.png)


    



```python

```


```python

```


```python
# 지도 학습
# 분류 - 이산형 - 훈련데이터의 레이블 중 하나로 분류
# 회귀 - 연속형 - 연속된 값을 예측
```


```python
# K-최근접 이웃
# 새로운 입력을 받았을 때 모든 데이터와 인스턴스 기반거리를 측정하여 가장 많은 속성을 가진 클러스터에 할당하는 분류
# 새로운 데이터와 인접한 K 개의 데이터를 찾아 더 많은 속성을 가진 클러스터에 할당
# K 값의 선택에 따라 새로운 데이터에 대한 분류 결과가 달라질 수도 있다.
```


```python

```


```python
from sklearn import metrics             # 모델 성능 평가 라이브러리
```


```python
import numpy as np                      # 벡터 및 행렬의 연산 처리를 위한 라이브러리
```


```python
import matplotlib.pyplot as plt         # 시각화 라이브러리
```


```python
import pandas as pd                     # 데이터 분석 및 조작을 위한 라이브러리
```


```python

```


```python
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
```


```python
data = pd.read_csv("C:/Users/user8/learning/source/chap3/data/iris.data", names=names)
```


```python
data                                     # 판다스 데이터 프레임
```




<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal-length</th>
      <th>sepal-width</th>
      <th>petal-length</th>
      <th>petal-width</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 5 columns</p>
</div>




```python
data.head()
```




<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal-length</th>
      <th>sepal-width</th>
      <th>petal-length</th>
      <th>petal-width</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
  </tbody>
</table>
</div>




```python
x = data.iloc[ : , :-1].values          # 주어진 데이터
y = data.iloc[ : , 4].values            # 예측할 데이터
```


```python

```


```python
from sklearn.model_selection import train_test_split
```


```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20)
```


```python
                                        # 검정데이터를 20%로 하여 학습데이터와 검정데이터를 분리해준다.
```


```python

```


```python
from sklearn.preprocessing import StandardScaler
```


```python
s = StandardScaler()                    # 특성 스케일링, 정규화를 시켜줌 (평균 = 0, 표준편차 = 1 )
```


```python
x_train = s.fit_transform(x_train)      # fit() 을 수행한 후 transform 수행 해줌.
x_test = s.fit_transform(x_test)        # 해당 데이터를 정규화를 시켜준다.
```


```python

```


```python
from sklearn.neighbors import KNeighborsClassifier
```


```python
kn = KNeighborsClassifier(n_neighbors=50)
```


```python
                                         # K = 50 인 K-최근접 이웃 모델 생성
```


```python
kn.fit(x_train, y_train)                 # 모델 훈련
```




<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=50)</pre></div></div></div></div></div>




```python

```


```python
from sklearn.metrics import accuracy_score
```


```python
y_pred = kn.predict(x_test)             # 검정데이터를 넣어 예측값 추출
print(f"정확도 : {accuracy_score(y_test, y_pred)}")
                                        # 추출된 예측값과 검정데이터를 비교하여 정확도 출력
```

    정확도 : 0.9
    


```python

```


```python
# 최적의 k 값 찾기
```


```python
k = 50                                  # 최대 K
acc_array = np.zeros(k)                 # k개의 0 으로 이루어진 array 생성

for i in np.arange(1, k+1, 1):         # 1부터 k+1 까지 1씩 증가. ( range와 같은 기능을 하지만 range와 달리 실수도 지원 )
    
    cl = KNeighborsClassifier(n_neighbors = i).fit(x_train, y_train)
                                        # k가 i 인 모델을 생성하여 훈련데이터로 모델 훈련
    y_pred = cl.predict(x_test)         # 예측값 추출
    acc = accuracy_score(y_test, y_pred)# 예측값과 검정값을 비교하여 정확도 추출
    
    acc_array[i-1] = acc                # 생성해둔 array에 저장
    
max_arr = np.amax(acc_array)            # 최대 정확도 추출
lst = list(acc_array)                   # index를 사용 하기 위해 array를 리스트로 변환
k = lst.index(max_arr) +1               # 최대 정확도의 인덱스에 +1

print(f"정확도 {max_arr * 100}% 으로 최적의 k는 {k+1} 입니다.")
```

    정확도 100.0% 으로 최적의 k는 3 입니다.
    


```python

```


```python
# 서포트 벡터 머신
# 분류를 위한 기준선을 정의하는 모델
# 새로운 입력이 들어오면 기준선을 기준으로 경계의 어느쪽에 해당하는지 분류
```


```python
# 서포트 벡터 : 결정경계와 가장 가까지 있는 데이터들. 경계를 정의하는 결정적인 역할
# 마진 : 결정경계와 서포트벡터의 사이의 거리
```


```python
# 하드 마진 : 이상치 허용 x
# 소프트 마진 : 이상치 허용 o
```


```python

```


```python
from sklearn import svm, metrics, datasets, model_selection
```


```python
import tensorflow as tf
```


```python
import os
```


```python
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'    # 로깅 제어
```


```python

```


```python
iris = datasets.load_iris()
x_train, x_test, y_train, y_test = model_selection.train_test_split(iris.data, iris.target, test_size = 0.6, random_state = 42)
```


```python

```


```python
svm  = svm.SVC(kernel='linear', C = 1.0, gamma = 0.5)
                                            # 선형 분류로 모델 생성
                                            # Noise가 많은 데이터라면 C를 작게, Noise가 별로 없는 데이터라면 C를 크게
                                            # C가 커지면 커질수록 하드, C가 작아질수록 소프트
                                            # gamma 가 높을 수록 훈련 데이터에 민감하게 반응 -> 높으면 결정경계가 곡선 형태를 띈다.
svm.fit(x_train, y_train)                   # 모델 훈련
```


```python

```




```python
pre = svm.predict(x_test)                  # 예측값 추출
print(accuracy_score(y_test, pre) * 100, "%")
                                           # 98.89%
```

    98.88888888888889 %
    


```python

```


```python
# 결정트리
# 이상치가 많은 값으로 구성된 데이터셋을 다룰 때 유용
```


```python
# 순도 : 범주 안에서 같은 종류의 데이터만 모여있는 정도
# 불순도 : 서로 다른 데이터가 섞여있는 정도
```


```python
# 엔트로피 : 확률변수의 불확실성을 수치로 나타낸 것
# 지니계수 : 불순도를 측정하는 지표
```


```python

```


```python
import pandas as pd
```


```python
df = pd.read_csv("C:/Users/user8/learning/source/chap3/data/titanic/train.csv", index_col = "PassengerId")
df = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Survived']]
                                                        # 사용할 데이터 전처리
df['Sex'] = df['Sex'].map({'male':0, 'female':1})       # 남자면 0 여자면 1
df = df.dropna()                                        # 결측치 제거

x = df.drop('Survived', axis = 1)                       
y = df['Survived']                                      
```


```python
df
```




<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Fare</th>
      <th>Survived</th>
    </tr>
    <tr>
      <th>PassengerId</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>0</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>3</td>
      <td>0</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>3</td>
      <td>1</td>
      <td>39.0</td>
      <td>0</td>
      <td>5</td>
      <td>29.1250</td>
      <td>0</td>
    </tr>
    <tr>
      <th>887</th>
      <td>2</td>
      <td>0</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>13.0000</td>
      <td>0</td>
    </tr>
    <tr>
      <th>888</th>
      <td>1</td>
      <td>1</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>890</th>
      <td>1</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0000</td>
      <td>1</td>
    </tr>
    <tr>
      <th>891</th>
      <td>3</td>
      <td>0</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.7500</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>714 rows × 7 columns</p>
</div>




```python
from sklearn.model_selection import train_test_split

xtr, xte, ytr, yte = train_test_split(x, y)        # 학습데이터, 검정데이터 생성
```


```python
from sklearn import tree

model = tree.DecisionTreeClassifier()              # 모델 생성
model.fit(xtr, ytr)                                # 모델 학습
```



```python
ypr = model.predict(xte)                           # 예측치 추출
```


```python
from  sklearn.metrics import accuracy_score

accuracy_score(yte, ypr) * 100                     # 정확도 추출 => 76.54 %
```




    76.53631284916202




```python
from sklearn.metrics import confusion_matrix      # 혼돈 매트릭스

pd.DataFrame(confusion_matrix(yte, ypr), columns = ['PNS', 'PS'], index = ["TNS", "TS"])
```




<div>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PNS</th>
      <th>PS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>TNS</th>
      <td>79</td>
      <td>21</td>
    </tr>
    <tr>
      <th>TS</th>
      <td>21</td>
      <td>58</td>
    </tr>
  </tbody>
</table>
</div>




```python

```
