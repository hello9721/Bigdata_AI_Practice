```python
# 딥러닝 용어

# 입력층 : 데이터를 받아들이는 층
# 은닉층 : 모든 입력 노드로부터 값을 받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달.
# 출력층 : 신경망의 최종 결괏값이 포함된 층
# 가중치 : 노드와 노드 간 연결 강도
# 바이어스 : 가중합을 더해주는 상수.
#            최종적으로 출력되는 값을 조절하는 역할.
# 가중합 (전달함수) : 가중치와 노드의 곱을 합한 것
# 활성화 함수 : 신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수
# 손실 함수 : 가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수
```


```python
# 활성화 함수

# 시그모이드 함수 : 선형 함수의 결과를 0 ~ 1 사이의 비선형 형태로 변형해준다.
#                   딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 기울기 소멸 문제가 있어서 딥러닝 모델에는 잘 사용하지 않음.

# 하이퍼볼릭 탄젠트 함수 : 선형 함수의 결과를 -1 ~ 1 사이의 비선형 형태로 변형
#                          시그모이드의 결과값 평균이 0이 아닌 양수로 편향되는 문제는 해결 했지만,
#                          여전히 기울기 소멸 문제가 남아 발생한다.

# ReLU 함수 : 입력이 음수일 때는 0 출력, 양수일 때는 x를 출력.
#             경사 하강법에 영향을 주지 않아 학습 속도가 빠르고 기울기 소멸 문제도 발생하지 않는 장점.
#             하지만 음수 값을 입력 받으면 항상 0을 출력하기에 학습 능력 감소.

# Leaky ReLU 함수 : 입력값이 음수일 때 0.001 같은 매우 작은 수 반환.
#                   ReLU 함수의 학습 능력 감소 문제 해결.

# 소프트맥스 함수 : 입력 값을 항상 0 ~ 1 상이에 출력되도록 정규화 하여 총합이 항상 1이 되도록 한다.
#                   출력 노드의 활성화 함수로 많이 사용됨.
```


```python
# 손실 함수

# 평균 제곱 오차 : 실제 값과 예측 값의 차이를 제곱하여 평균을 내는 것.
#                  이 값이 작을 수록 예측력이 좋다.

# 크로스 엔트로피 오차 : 분류 문제에서 원-핫 인코딩을 챘을 때만 사용할 수 있는 오차 계산법.
```


```python
# 딥러닝 학습

# 순전파 : 훈련데이터가 들어올 때 발생.
#          데이터를 기반으로 예측 값을 계산하기 위해 긴경망을 교차해서 지나간다.

# 역전파 : 손실 함수 비용이 이상적인 0에 가깝도록 하기 위해 발생.
#          손실 함수를 통해 계산된 손실을 역으로 전파하여 은닉층의 각 뉴런이 기여한 상대적 기여도에 따라 값이 조정이 된다. 
```


```python
# 딥러닝 문제점

# 과적합 문제 : 훈련데이터를 과하게 학습하여 발생.
#               훈련데이터에서의 오차는 감소했지만, 검증 데이터에 대해서는 오차가 증가.
# 드롭 아웃 : 과적합 되는 것을 막기위해 학습 과정 중 임의로 일부 노드들을 학습에서 제외 시키는 것.

# 기울기 소멸 문제 : 은닉층이 많은 신경망에서 주로 발생.
#                    출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상.
#                    즉, 기울기가 소멸되어 학습되는 양이 0에 가가워져 학습이 더디게 진행되다가
#                    오차를 더 줄이지 못하고 그 상태로 수렴하는 현상.
# 시그모이드나 하이퍼볼릭 탄젠트 대신 ReLU함수를 사용하면 해결됨.

# 성능이 나빠지는 문제 : 경사 하강법은 손실 함수 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동하는데
#                        이 때 성능이 나빠지는 문제가 발생한다.
# 미니 배치 경사 하강법 : 전체 데이터 셋을 미니 배치 여러개로 나누고, 미니 배치 한 개마다 기울기를 구한 후
#                         그것의 평균 기울기를 이용하여 모델을 업데이트해서 학습.
#                         경사 하강법 보다 빠르고, 확률적 경사 하강법보다 안정적이라 가장 많이 사용.
# 확률적 경사 하강법 : 임의로 선택한 데이터에 대해 기울기를 계산하는 방법.
#                      적은 데이터로 빠른 계산이 가능 하지만 불안정하다.
```


```python
# 알고리즘

# DNN ( 심층 신경망 )
# 입력층과 출력층 사이에 다수의 은닉층을 포함하는 인공 신경망
# 다양한 비선형적 관계를 학습할 수 있지만, 연산량이 많고 기울기 소멸문제 등이 발생할 수 있음.

# CNN ( 합성곱 신경망 )
# 합성곱층과 풀링층을 포함하는 이미지 처리 성능이 좋은 인공 신경망
# 이미지에서 패턴을 찾는 것에 유용.
# LeNet-5, AlexNet가 대표적. 더 층을 깊게 쌓은 신경망으로는 VGG, GoogLeNet, ResNet ...
# 기존 신경망에 비해 입출력 형상을 유지.
# 이미지의 공간정보를 유지하면서 인접 이미지와 차이가 있는 특징을 효과적으로 인식.
# 복수 필터로 이미지의 특징을 추출하고 학습.
# 필터를 공유 파라미터로 사용하기에 학습 파라미터가 적다.
# 풀링층 : 추출한 이미지의 특징을 모으고 강화함.

# RNN ( 순환 신경망 )
# 시계열 데이터 ( 음악 및 영상 등등 ) 같이 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망
# 시간에 따라 내용이 변하므로 데이터는 동적이고 길이가 가변적.
```
